{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Recurrent_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lQJ8zMLgmT3"
      },
      "source": [
        "\n",
        "Recurrent neural networks(RNN) with long short-term memory(LSTM)\n",
        "Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\n",
        "\n",
        "As powerful as convolutional neural networks (CNNs) are, they don't handle sequential data so well; however, they are great for non-sequential tasks, such as image classification.\n",
        "\n",
        "CNN.PNG\n",
        "\n",
        "Recurrent neural networks (RNNs), which really are state of the art, can handle sequential tasks. An RNN consists of CNNs where data is received in a sequence.\n",
        "\n",
        "rnn.PNG\n",
        "\n",
        "Data coming in a sequence (xi) goes through the neural network and we get an output (yi). The output is then fed through to another iteration and forms a loop. In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other.\n",
        "\n",
        "Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition, language translation, video identification, and text generation.\n",
        "\n",
        "rnn2.png\n",
        "\n",
        "This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.\n",
        "\n",
        "First, it takes the X(0) from the sequence of input and then it outputs h(0) which together with X(1) is the input for the next step. So, the h(0) and X(1) is the input for the next step. Similarly, h(1) from the next is the input with X(2) for the next step and so on. This way, it keeps remembering the context while training.\n",
        "\n",
        "Advantages of Recurrent Neural Network\n",
        "RNN can model sequence of data so that each sample can be assumed to be dependent on previous ones\n",
        "Recurrent neural network are even used with convolutional layers to extend the effective pixel neighbourhood.\n",
        "Disadvantages of Recurrent Neural Network\n",
        "Gradient vanishing and exploding problems.\n",
        "Training an RNN is a very difficult task.\n",
        "It cannot process very long sequences if using tanh or relu as an activation function.\n",
        "What is Long Short Term Memory (LSTM)?\n",
        "Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:\n",
        "\n",
        "LSTM.jpeg\n",
        "\n",
        "Input gate — discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.\n",
        "\n",
        "Forget gate — discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state (ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct−1\n",
        "\n",
        "Output gate — the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YIjKkECgfcV"
      },
      "source": [
        "#import required packages/library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM#, CuDNNLSTM"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wKrc1VnhIev"
      },
      "source": [
        "\n",
        "Similar to before, we load in our data, and we can see the shape again of the dataset and individual samples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkggK4oPgizQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qclltJ4egjOF",
        "outputId": "851c505c-e055-4109-a530-eb1005799b63"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
        "\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "print(\"x_train =\",x_train.shape)\n",
        "print(\"y_train =\",y_train.shape)\n",
        "print(\"x_test =\",x_test.shape)\n",
        "print(\"y_test =\",y_test.shape)\n",
        "\n",
        "print(x_train[0].shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train = (60000, 28, 28)\n",
            "y_train = (60000,)\n",
            "x_test = (10000, 28, 28)\n",
            "y_test = (10000,)\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timnkuZvhRdF"
      },
      "source": [
        "Recall we had to flatten this data for the regular deep neural network. In this model, we're passing the rows of the image as the sequences. So basically, we're showing the the model each pixel row of the image, in order, and having it make the prediction. (28 sequences of 28 elements)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPhZjj0xhS4N"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# IF you are running with a GPU, try out the CuDNNLSTM layer type instead (don't pass an activation, tanh is required)\n",
        "model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#return_sequences= True   # This flag is used for when you're continuing on to another recurrent layer.\n",
        "\n",
        "model.add(LSTM(128, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(10, activation='softmax')) #output layer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1w3oAM0ha1R"
      },
      "source": [
        "This should all be straight forward, where rather than Dense or Conv, we're just using LSTM as the layer type. The only new thing is return_sequences. This flag is used for when you're continuing on to another recurrent layer. If you are, then you want to return sequences. If you're not going to another recurrent-type of layer, then you don't set this to true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHd_PoKahc-L",
        "outputId": "04889e65-8673-418b-dba8-d82d7d8917d4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 28, 128)           80384     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 28, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 216,426\n",
            "Trainable params: 216,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ty0AIc7hsWY"
      },
      "source": [
        "Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dsVvDB0htDo"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkpkdp0vhy1c"
      },
      "source": [
        "Training to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0YaFhZ_hz5i",
        "outputId": "953cbddd-d7c7-4ce6-b7b6-f86a71b80b33"
      },
      "source": [
        "model.fit(x_train,y_train,\n",
        "          epochs=3,verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 114s 60ms/step - loss: 1.0050 - accuracy: 0.6556\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 113s 60ms/step - loss: 0.1650 - accuracy: 0.9543\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 115s 62ms/step - loss: 0.1112 - accuracy: 0.9709\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f80d0c2c450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bmIQmath9Ci",
        "outputId": "4fb65778-47b0-4254-abde-c6cfa36137e3"
      },
      "source": [
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 6s 19ms/step - loss: 0.0718 - accuracy: 0.9784\n",
            "Test score: 0.07184668630361557\n",
            "Test accuracy: 0.9783999919891357\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}